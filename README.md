# ğŸ¯ Context Engineering Sandbox

> A demonstration project showcasing the progressive gains of context engineering techniques in LLM applications using Google ADK with local Ollama models.

## ğŸ“‹ Project Overview

This project is a **modular experimentation platform** for context engineering techniques. Instead of linear phase progression, it enables dynamic comparison of different configurations through a toggleable architecture, measuring the impact of each technique on:
- **Effectiveness**: Answer accuracy, relevance, hallucination rate
- **Efficiency**: Latency, token usage, cost
- **Scalability**: Throughput, memory usage

**Key Innovation**: Run the same query with different context engineering techniques enabled/disabled, then compare results side-by-side to measure real impact.

## ğŸš€ Technology Stack

### Backend
- **LLM Framework**: Google ADK v1.17.0 (Agentic Development Kit)
- **Local LLM**: Ollama with Qwen3 4B (2.5 GB)
- **Model Integration**: LiteLLM v1.72.6
- **API Framework**: FastAPI + Uvicorn with WebSocket support
- **Vector Database**: ChromaDB (local, persistent) - Phase 2+
- **Embeddings**: sentence-transformers (local) - Phase 2+
- **Language**: Python 3.11+

### Frontend (Phase 1.5+)
- **Framework**: React 18 + TypeScript
- **Build Tool**: Vite
- **UI Library**: Shadcn/UI (built on Tailwind CSS)
- **Agent Protocol**: AG-UI (CopilotKit)
- **Charts**: Recharts
- **HTTP Client**: Axios
- **Routing**: React Router

## ğŸ“Š Project Status

**Current Phase**: Phase 1.5 - Web UI Development âœ… **COMPLETE**

### Phase 0 Baseline Metrics

| Metric | Value |
|--------|-------|
| ROUGE-1 F1 (mean) | 0.3149 |
| ROUGE-2 F1 (mean) | 0.1598 |
| ROUGE-L F1 (mean) | 0.2509 |
| Relevance Score (mean) | 0.5698 |
| Hallucination Rate (mean) | 0.0422 |
| Latency (ms) | ~0 |
| Tokens/Query | 29.27 |

## ğŸ—ºï¸ Implementation Phases

- âœ… **Phase 0**: Foundation & Benchmarking (Complete)
- âœ… **Phase 1**: MVP Agent with Google ADK (Complete)
- âœ… **Phase 1.5**: Web UI Development (Complete)
- â³ **Phase 2**: Modular Platform Infrastructure (Next - Build toggleable architecture!)
- â³ **Phase 3**: RAG Module (First technique: vector retrieval)
- â³ **Phase 4**: Compression, Caching & Memory Modules (Efficiency techniques)
- â³ **Phase 5**: Reranking & Hybrid Search Modules (Quality techniques)
- â³ **Phase 6**: Advanced Technique Modules (Graph RAG, adaptive chunking)
- â³ **Phase 7**: System Integration & Optimization

**Architecture Shift**: Each phase after Phase 2 implements ONE technique as a pluggable module that can be toggled and configured independently.

## ğŸ—ï¸ Project Structure

```
context-engineering-sandbox/
â”œâ”€â”€ frontend/              # Phase 1.5: React Web UI
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/   # React components (UI, chat, metrics)
â”‚   â”‚   â”œâ”€â”€ pages/        # Route pages
â”‚   â”‚   â”œâ”€â”€ hooks/        # Custom React hooks
â”‚   â”‚   â”œâ”€â”€ services/     # API services
â”‚   â”‚   â””â”€â”€ types/        # TypeScript types
â”‚   â””â”€â”€ public/           # Static assets
â”œâ”€â”€ src/                   # Backend Python source
â”‚   â”œâ”€â”€ core/             # Core configuration and utilities
â”‚   â”œâ”€â”€ evaluation/       # Metrics and benchmarking framework
â”‚   â”œâ”€â”€ retrieval/        # Vector search and RAG (Phase 2+)
â”‚   â”œâ”€â”€ memory/           # Conversation memory (Phase 4+)
â”‚   â”œâ”€â”€ compression/      # Context compression (Phase 5+)
â”‚   â”œâ”€â”€ advanced/         # Graph RAG, routing (Phase 6+)
â”‚   â””â”€â”€ api/              # FastAPI endpoints (Phase 1.5+)
â”œâ”€â”€ context_engineering_agent/  # ADK agent directory
â”œâ”€â”€ configs/              # YAML configuration files
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ test_sets/        # Benchmark datasets
â”‚   â””â”€â”€ knowledge_base/   # Documents for RAG
â”œâ”€â”€ docker/               # Docker configuration
â”œâ”€â”€ scripts/              # Utility scripts
â”œâ”€â”€ docs/                 # Documentation
â””â”€â”€ tests/                # Test suites
```

## ğŸš€ Quick Start

### 1. Prerequisites

**Required:**
- Python 3.11+
- [Ollama](https://ollama.com) installed and running

**Install Ollama:**
```bash
# Download from https://ollama.com
# Or on macOS:
brew install ollama

# Start Ollama service (if not auto-started)
ollama serve
```

### 2. Setup Environment

```bash
# Clone the repository
git clone https://github.com/RevAI-Workgroup/ADK-ContextEngineering.git
cd ADK-ContextEngineering

# Create virtual environment
python -m venv venv

# Activate (Windows)
.\venv\Scripts\Activate.ps1

# Activate (Linux/Mac)
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 3. Download the LLM Model

```bash
# Download Qwen3 4B model (2.5 GB) - takes 1-2 minutes
ollama pull qwen3:4b

# Verify model is available
ollama list
```

### 4. Run the Application

**Option A: Docker (Recommended - Full Stack):**
```bash
# Build and run both frontend and backend
docker-compose -f docker/docker-compose.yml up --build

# Access the web UI at http://localhost
```

**Option B: Development Mode (Backend + Frontend):**

Terminal 1 - Backend:
```bash
# Start FastAPI backend
uvicorn src.api.main:app --reload --host 0.0.0.0 --port 8000
```

Terminal 2 - Frontend:
```bash
# Install frontend dependencies (first time only)
cd frontend
npm install

# Start frontend dev server
npm run dev

# Access the web UI at http://localhost:5173
```

**Option C: CLI Only (No Web UI):**
```bash
# Interactive mode
adk run context_engineering_agent

# Single query
echo "What is 5 plus 3?" | adk run context_engineering_agent

# ADK's built-in web interface
adk web
```

### 5. Run Tests (Optional)

**Manual Test Script:**
```bash
./scripts/test_adk_agent.sh
# Tests all 4 tools with example queries
```

**Phase 0 Baseline (For Comparison):**
```bash
# Generate benchmark datasets (if not already done)
python scripts/create_benchmarks.py

# Run Phase 0 baseline evaluation
python scripts/run_evaluation.py
```

## ğŸ“ˆ Evaluation Framework

The project includes a comprehensive evaluation framework:

- **Metrics Collection**: Automated tracking of accuracy, efficiency, and quality metrics
- **Benchmark Datasets**: 15 diverse test cases across technical, general, reasoning, and factual categories
- **Paired Comparison Testing**: Framework for comparing techniques on identical test cases to measure relative gains
- **Results Storage**: Detailed JSON reports with aggregate statistics

## ğŸ”§ Configuration

All configuration is managed through YAML files in the `configs/` directory:

- `models.yaml` - LLM and embedding model configuration
- `retrieval.yaml` - Vector search and chunking strategies
- `evaluation.yaml` - Metrics and benchmarking settings

**Environment Variable Override**: Any config value can be overridden via environment variables with automatic type conversion:
```bash
# YAML: models.ollama.timeout: 120 (int)
export MODELS_OLLAMA_TIMEOUT=240  # Automatically converted to int 240

# YAML: models.ollama.enabled: true (bool)
export MODELS_OLLAMA_ENABLED=yes  # Automatically converted to bool True
```

## ğŸ“š Documentation

- **[BACKLOG.md](BACKLOG.md)** - Detailed implementation plan with modular architecture
- **[.context/](.context/)** - AI assistant context files
- **[docs/phase_summaries/](docs/phase_summaries/)** - Phase completion reports

### Experimentation Workflow

The platform enables systematic comparison of context engineering techniques:

1. **Configure**: Toggle techniques on/off, adjust parameters, select presets
2. **Run**: Execute query with current configuration (automatically saved to history)
3. **Store**: Last 8 runs kept in history with full context
4. **Compare**: Select multiple runs to see side-by-side differences
5. **Analyze**: View metric deltas in Metrics dashboard
6. **Iterate**: Refine configuration based on insights

**Example**: Run "What is RAG?" three times:
- Run 1: Baseline (no techniques) â†’ Accuracy: 0.85
- Run 2: +RAG enabled â†’ Accuracy: 0.95 (+10%)
- Run 3: +RAG +Compression â†’ Accuracy: 0.93, Tokens: -22%

Compare all three to see which configuration offers the best tradeoff.

## ğŸ”¬ Current Phase: Phase 1 âœ… COMPLETE

Phase 1 integrates Google ADK with Ollama backend:

### âœ… Completed
- Google ADK v1.17.0 + LiteLLM v1.72.6 integration
- Ollama backend configured with Qwen3 4B model (2.5 GB)
- ADK agent directory structure: `context_engineering_agent/`
- 4 working tools implemented and tested:
  - `calculate` - Safe arithmetic (AST-based)
  - `count_words` - Word counting
  - `get_current_time` - Timezone-aware queries
  - `analyze_text` - Comprehensive text analysis
- Agent successfully runs via `adk run context_engineering_agent`
- Tool calling verified: Agent correctly selects and executes tools
- Comprehensive documentation: [Phase 1 Summary](docs/phase_summaries/phase1_summary.md)

### ğŸ§ª Test Results
- âœ… Calculator: `5 + 3 = 8`, `123 / 4 = 30.75`
- âœ… Word counter: Correctly counted 6 words
- âœ… Time tool: Retrieved America/New_York time
- âœ… Agent reasoning: Shows clear decision-making process

### ğŸ¯ Key Decisions
- **Skipped**: File system & code execution tools (not needed for context engineering)
- **Skipped**: Custom FastAPI (use ADK's built-in `adk web` and `adk run`)
- **Deferred**: Web search to Phase 3 (external context retrieval)

## ğŸ¨ Phase 1.5: Web UI Development âœ… COMPLETE

Phase 1.5 delivers a modern React frontend with AG-UI integration:

### âœ… Completed
- **Frontend**: React 18 + TypeScript with Vite
- **UI Framework**: Shadcn/UI components on Tailwind CSS
- **AG-UI Protocol**: CopilotKit integration for agent-user interaction
- **Real-time Communication**: WebSocket support for streaming responses
- **Pages**: Home, Chat, Metrics dashboard with charts
- **Backend API**: FastAPI with WebSocket endpoints
- **Docker**: Multi-container setup with Nginx reverse proxy
- **Documentation**: Comprehensive [Phase 1.5 Summary](docs/phase_summaries/phase1_5_summary.md)

### ğŸ¯ Key Features
- Interactive chat interface with thinking visualization
- Real-time metrics dashboard with phase comparisons
- Tool call display and agent reasoning transparency
- Responsive design with dark mode support
- WebSocket streaming for live agent updates

## ğŸ”® Next Steps: Phase 2 - Modular Platform Infrastructure

**Building the foundation for experimentation!**

Phase 2 builds the infrastructure that makes context engineering techniques toggleable and comparable:

### Backend Infrastructure
- **Configuration System**: Dataclass with toggles for 6 techniques (RAG, Compression, Reranking, Caching, Hybrid Search, Memory)
- **Run History**: Track last 8 runs with query, config, response, and metrics
- **Modular Pipeline**: Base class for all technique modules + orchestrator
- **API Endpoints**: `/api/runs`, `/api/config`, `/api/runs/compare`

### Frontend Components
- **Configuration Panel**: Simple toggles + advanced settings for each technique
- **Run History Sidebar**: View and select from last 8 runs
- **Run Comparison Modal**: Side-by-side comparison with metric deltas
- **Updated Metrics Page**: Compare selected runs instead of sequential phases

### Why This Matters
This phase transforms the application from "what does Phase 3 deliver?" to "what happens when I turn RAG on?" - enabling true experimentation with context engineering techniques.

**After Phase 2**, each subsequent phase will implement ONE technique module that plugs into this infrastructure.

## ğŸ¤ Contributing

This is a research and demonstration project. Contributions are welcome:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests and benchmarks
5. Submit a pull request

## ğŸ“ License

[MIT License](LICENSE)

## ğŸ™ Acknowledgments

- Google ADK team
- Ollama project
- ChromaDB
- Sentence-Transformers
- FastAPI

---

## ğŸ§ª Advanced Testing

### Testing Individual Tools

**Calculator Tool:**
```bash
echo "What is 123 divided by 4?" | adk run context_engineering_agent
# Expected: Uses calculate tool, returns 30.75
```

**Word Counter Tool:**
```bash
echo "Count the words in: The quick brown fox jumps over the lazy dog" | adk run context_engineering_agent
# Expected: Uses count_words tool, returns 9 words
```

**Time Tool:**
```bash
echo "What's the current time in Asia/Tokyo?" | adk run context_engineering_agent
# Expected: Uses get_current_time tool, returns current JST time
```

**Text Analysis Tool:**
```bash
echo "Analyze this text: Python is an amazing programming language" | adk run context_engineering_agent
# Expected: Uses analyze_text tool, returns character count, word count, etc.
```

### Automated Test Suite

Run all tool tests automatically:
```bash
chmod +x scripts/test_adk_agent.sh
./scripts/test_adk_agent.sh
```

## ğŸ”§ Troubleshooting

### Issue: "Command not found: adk"
**Solution:** Make sure Google ADK is installed:
```bash
pip install google-adk
adk --version  # Should show v1.17.0
```

### Issue: "Model not found: qwen3:4b"
**Solution:** Download the model:
```bash
ollama pull qwen3:4b
ollama list  # Verify it's downloaded
```

### Issue: "Connection refused to Ollama"
**Solution:** Ensure Ollama is running:
```bash
# Check if Ollama is running
ps aux | grep ollama

# If not, start it:
ollama serve
```

### Issue: Agent responds slowly
**Expected:** First response takes ~30-45 seconds (model reasoning + tool calling).
Subsequent responses are similar. This is normal for a 4B parameter model.

### Issue: "Directory 'context_engineering_agent' does not exist"
**Solution:** Make sure you're in the project root directory:
```bash
cd ADK-ContextEngineering
ls context_engineering_agent/  # Should show agent.py and __init__.py
```

---

**Note**: This project uses local models for zero-cost experimentation. All metrics are reproducible on consumer hardware.

*Last Updated: Phase 1.5 Complete - Architecture Shift to Modular Platform - 2025-11-03*
